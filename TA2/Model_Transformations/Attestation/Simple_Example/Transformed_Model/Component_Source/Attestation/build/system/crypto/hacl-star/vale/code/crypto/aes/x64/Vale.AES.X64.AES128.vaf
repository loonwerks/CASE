include "../../../arch/x64/Vale.X64.InsBasic.vaf"
include "../../../arch/x64/Vale.X64.InsMem.vaf"
include "../../../arch/x64/Vale.X64.InsVector.vaf"
include "../../../arch/x64/Vale.X64.InsAes.vaf"
include{:fstar}{:open} "Vale.Def.Opaque_s"
include{:fstar}{:open} "Vale.Def.Types_s"
include{:fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "Vale.AES.AES_s"
include{:fstar}{:open} "Vale.X64.Machine_s"
include{:fstar}{:open} "Vale.X64.Memory"
include{:fstar}{:open} "Vale.X64.State"
include{:fstar}{:open} "Vale.X64.Decls"
include{:fstar}{:open} "Vale.X64.QuickCode"
include{:fstar}{:open} "Vale.X64.QuickCodes"
include{:fstar}{:open} "Vale.Arch.Types"
include{:fstar}{:open} "Vale.AES.AES_helpers"
include{:fstar}{:open} "Vale.X64.CPU_Features_s"

module Vale.AES.X64.AES128

#verbatim{:interface}{:implementation}
open Vale.Def.Opaque_s
open Vale.Def.Types_s
open FStar.Seq
open Vale.AES.AES_s
open Vale.X64.Machine_s
open Vale.X64.Memory
open Vale.X64.State
open Vale.X64.Decls
open Vale.X64.InsBasic
open Vale.X64.InsMem
open Vale.X64.InsVector
open Vale.X64.InsAes
open Vale.X64.QuickCode
open Vale.X64.QuickCodes
open Vale.Arch.Types
open Vale.AES.AES_helpers
open Vale.X64.CPU_Features_s
#reset-options "--z3rlimit 20"
#endverbatim

///////////////////////////
// KEY EXPANSION
///////////////////////////

// Given round key for round, generate round key for round + 1
procedure KeyExpansionRound(
        inline round:nat64,
        inline rcon:nat8,
        ghost dst:buffer128,
        ghost key:seq(nat32))
    {:quick}
    reads
        rdx; memLayout;
    modifies
        heap1; xmm1; xmm2; xmm3; efl;
    requires/ensures
        validDstAddrs128(heap1, rdx, dst, 11, memLayout, Secret);
    requires
        aesni_enabled && avx_enabled && sse_enabled;
        0 <= round < 10;
        rcon == aes_rcon(round);
        is_aes_key_LE(AES_128, key);
        xmm1 == expand_key_128(key, round);
    ensures
        xmm1 == buffer128_read(dst, round + 1, heap1);
        xmm1 == expand_key_128(key, round + 1);
        modifies_buffer_specific128(dst, old(heap1), heap1, round + 1, round + 1);
{
    AESNI_keygen_assist(xmm2, xmm1, rcon);
    Pshufd(xmm2, xmm2, 255);
    VPslldq4(xmm3, xmm1);
    Pxor(xmm1,xmm3);
    VPslldq4(xmm3, xmm1);
    Pxor(xmm1,xmm3);
    VPslldq4(xmm3, xmm1);
    Pxor(xmm1,xmm3);
    Pxor(xmm1,xmm2);
    Store128_buffer(heap1, rdx, xmm1, 16 * (round + 1), Secret, dst, round + 1);

    // assert xmm1 == simd_round_key_128(old(xmm1), rcon);
    lemma_simd_round_key(old(xmm1), rcon);
    expand_key_128_reveal();
}

procedure KeyExpansionRoundUnrolledRecursive(
        ghost key:seq(nat32),
        ghost dst:buffer128,
        inline n:int)
    {:decrease n}
    {:recursive}
    {:quick exportOnly}
    reads
        rdx; memLayout;
    modifies
        heap1; xmm1; xmm2; xmm3; efl;
    requires/ensures
        validDstAddrs128(heap1, rdx, dst, 11, memLayout, Secret);
    requires
        aesni_enabled && avx_enabled && sse_enabled;
        0 <= n <= 10;
        is_aes_key_LE(AES_128, key);
        key == quad32_to_seq(xmm1);
        xmm1 == buffer128_read(dst, 0, heap1);
        rdx == buffer_addr(dst, heap1);
    ensures
        modifies_buffer128(dst, old(heap1), heap1);
        xmm1 == buffer128_read(dst, n, heap1);
        forall(j){buffer128_read(dst, j, heap1)} 0 <= j <= n ==>
            buffer128_read(dst, j, heap1) == expand_key_128(key, j);
{
    expand_key_128_reveal();
    inline if (0 < n <= 10) {
        KeyExpansionRoundUnrolledRecursive(key, dst, n - 1);
        KeyExpansionRound(#nat64(n - 1), #nat8(aes_rcon(n - 1)), dst, key);
    }
}

procedure KeyExpansion128Stdcall(
        inline win:bool,
        ghost input_key_b:buffer128,
        ghost output_key_expansion_b:buffer128)
    {:public}
    {:quick}
    reads
        rcx; rsi; rdi; memLayout; heap0;
    modifies
        rdx;
        heap1; xmm1; xmm2; xmm3; efl;
    lets
        key_ptr := if win then rcx else rdi;
        key_expansion_ptr := if win then rdx else rsi;
        key := quad32_to_seq(buffer128_read(input_key_b, 0, heap0));
    requires/ensures
        aesni_enabled && avx_enabled && sse_enabled;
        validSrcAddrs128(heap0, key_ptr, input_key_b, 1, memLayout, Secret);
        validDstAddrs128(heap1, key_expansion_ptr, output_key_expansion_b, 11, memLayout, Secret);
    ensures
        modifies_buffer128(output_key_expansion_b, old(heap1), heap1);
        forall(j){buffer128_read(output_key_expansion_b, j, heap1)} 0 <= j <= 10 ==>
            buffer128_read(output_key_expansion_b, j, heap1) == index(key_to_round_keys_LE(AES_128, key), j);
{
    inline if (win)
    {
        Load128_buffer(heap0, xmm1, rcx, 0, Secret, input_key_b, 0);
    }
    else
    {
        Load128_buffer(heap0, xmm1, rdi, 0, Secret, input_key_b, 0);
        Mov64(rdx, rsi);
    }

    Store128_buffer(heap1, rdx, xmm1, 0, Secret, output_key_expansion_b, 0);
    KeyExpansionRoundUnrolledRecursive(key, output_key_expansion_b, 10);
    lemma_expand_key_128(key, 11);
    reveal key_to_round_keys_LE;

    // Clear secrets out of registers
    Pxor(xmm1, xmm1);
    Pxor(xmm2, xmm2);
    Pxor(xmm3, xmm3);
}

///////////////////////////
// ENCRYPTION
///////////////////////////

procedure AES128EncryptRound(
        inline n:nat,
        ghost init:quad32,
        ghost round_keys:seq(quad32),
        ghost keys_buffer:buffer128)
    {:quick}
    reads
        r8; heap0; memLayout;
    modifies
        xmm0; xmm2; efl;
    requires
        aesni_enabled && sse_enabled;
        1 <= n < 10 <= length(round_keys);
        xmm0 == eval_rounds_def(init, round_keys, #nat(n - 1));
        r8 == buffer_addr(keys_buffer, heap0);
        validSrcAddrs128(heap0, r8, keys_buffer, 11, memLayout, Secret);
        buffer128_read(keys_buffer, n, heap0) == index(round_keys, n);
    ensures
        xmm0 == eval_rounds_def(init, round_keys, n);
{
    commute_sub_bytes_shift_rows(xmm0);
    Load128_buffer(heap0, xmm2, r8, 16 * n, Secret, keys_buffer, n);
    AESNI_enc(xmm0, xmm2);
}

procedure AES128EncryptBlock(
        ghost input:quad32,
        ghost key:seq(nat32),
        ghost round_keys:seq(quad32),
        ghost keys_buffer:buffer128)
    {:public}
    {:quick}
    reads
        r8; heap0; memLayout;
    modifies
        xmm0; xmm2; efl;
    requires
        aesni_enabled && sse_enabled;
        is_aes_key_LE(AES_128, key);
        length(round_keys) == 11;
        round_keys == key_to_round_keys_LE(AES_128, key);
        xmm0 == input;
        r8 == buffer_addr(keys_buffer, heap0);
        validSrcAddrs128(heap0, r8, keys_buffer, 11, memLayout, Secret);
        forall(i:nat) i < 11 ==> buffer128_read(keys_buffer, i, heap0) == index(round_keys, i);
    ensures
        xmm0 == aes_encrypt_LE(AES_128, key, input);
{
    let init := quad32_xor(input, index(round_keys, 0));

    Load128_buffer(heap0, xmm2, r8, 0, Secret, keys_buffer, 0);
    Pxor(xmm0, xmm2);
    AES128EncryptRound(1, init, round_keys, keys_buffer);
    AES128EncryptRound(2, init, round_keys, keys_buffer);
    AES128EncryptRound(3, init, round_keys, keys_buffer);
    AES128EncryptRound(4, init, round_keys, keys_buffer);
    AES128EncryptRound(5, init, round_keys, keys_buffer);
    AES128EncryptRound(6, init, round_keys, keys_buffer);
    AES128EncryptRound(7, init, round_keys, keys_buffer);
    AES128EncryptRound(8, init, round_keys, keys_buffer);
    AES128EncryptRound(9, init, round_keys, keys_buffer);
    commute_sub_bytes_shift_rows(xmm0);
    Load128_buffer(heap0, xmm2, r8, 16 * 10, Secret, keys_buffer, 10);
    AESNI_enc_last(xmm0, xmm2);

    // Clear secrets out of registers
    Pxor(xmm2, xmm2);
    aes_encrypt_LE_reveal();
}

procedure AES128EncryptBlockStdcall(
        inline win:bool,
        ghost input:quad32,
        ghost key:seq(nat32),
        ghost input_buffer:buffer128,
        ghost output_buffer:buffer128,
        ghost keys_buffer:buffer128)
    {:public}
    {:quick}
    reads
        rcx; rdx; rsi; rdi; memLayout; heap0;
    modifies
        r8;
        heap1; xmm0; xmm2; efl;
    lets
        output_ptr := if win then rcx else rdi;
        input_ptr := if win then rdx else rsi;
        expanded_key_ptr := if win then r8 else rdx;
    requires
        aesni_enabled && sse_enabled;
        is_aes_key_LE(AES_128, key);
        buffer128_read(input_buffer, 0, heap0) == input;
        expanded_key_ptr == buffer_addr(keys_buffer, heap0);
        validSrcAddrs128(heap0, input_ptr, input_buffer, 1, memLayout, Secret);
        validDstAddrs128(heap1, output_ptr, output_buffer, 1, memLayout, Secret);
        validSrcAddrs128(heap0, expanded_key_ptr, keys_buffer, 11, memLayout, Secret);
        forall(i:nat) i < 11 ==>
            buffer128_read(keys_buffer, i, heap0) == index(key_to_round_keys_LE(AES_128, key), i);
    ensures
        modifies_mem(loc_buffer(output_buffer), old(heap1), heap1);
        validSrcAddrs128(heap1, output_ptr, output_buffer, 1, memLayout, Secret);
        buffer128_read(output_buffer, 0, heap1) == aes_encrypt_LE(AES_128, key, input);
{
    inline if (win)
    {
        Load128_buffer(heap0, xmm0, rdx, 0, Secret, input_buffer, 0);
    }
    else
    {
        Load128_buffer(heap0, xmm0, rsi, 0, Secret, input_buffer, 0);
        Mov64(r8, rdx);
    }

    AES128EncryptBlock(input, key, key_to_round_keys_LE(AES_128, key), keys_buffer);

    inline if (win)
    {
        Store128_buffer(heap1, rcx, xmm0, 0, Secret, output_buffer, 0);
    }
    else
    {
        Store128_buffer(heap1, rdi, xmm0, 0, Secret, output_buffer, 0);
    }
}

/*
///////////////////////////
// KEY INVERSION
///////////////////////////

procedure KeyInversionRound(
    inline round:int,
//    inline taint:taint,
    ghost k_b:buffer128)
//    ghost key:aes_key_LE(AES_128),
//    ghost w:seq(nat32),
//    ghost dw_in:seq(nat32)
//    ) returns (
//    ghost dw_out:seq(nat32)
//    )
    requires/ensures
        validSrcAddrs128(heap0, rdx, k_b, 11);
    requires
        0 <= round <= 8;
//        SeqLength(w) == 44;
//        SeqLength(dw_in) == 4*(round+1);
//        KeyExpansionPredicate(key, AES_128, w);
//        EqInvkey_expansion_partial(key, AES_128, dw_in, round);
//        forall(j) 0 <= j <= round ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(dw_in[4*j], dw_in[4*j+1], dw_in[4*j+2], dw_in[4*j+3]);
//        forall(j) round < j <= 10 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
    reads
        rdx;
    modifies
        heap0; xmm1; efl;
    ensures
        modifies_buffer_specific128(k_b, old(heap0), heap0, round+1, round+1);
//        forall(a) (a < rdx || a >= rdx + 176) && old(heap0)[k_b].quads?[a] ==> heap0[k_b].quads?[a] && heap0[k_b].quads[a] == old(heap0)[k_b].quads[a];
//        forall(j) round+1 < j <= 10 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
//        SeqLength(dw_out) == 4*(round + 2);
//        forall(j) 0 <= j <= round+1 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(dw_out[4*j], dw_out[4*j+1], dw_out[4*j+2], dw_out[4*j+3]);
//        EqInvkey_expansion_partial(key, AES_128, dw_out, round+1);
{
//    assert heap0[k_b].quads[rdx + 16*(round+1)].v == Quadword(w[(round+1)*4], w[(round+1)*4 + 1], w[(round+1)*4 + 2], w[(round+1)*4 + 3]);

    Load128_buffer(heap0, xmm1, rdx, 16*(round+1), k_b, round+1);
//    assert xmm1 == Quadword(w[(round+1)*4], w[(round+1)*4 + 1], w[(round+1)*4 + 2], w[(round+1)*4 + 3]);
//    let ws := SeqRange(w, (round+1)*4, (round+1)*4 + 4);
//    assert ws[0] == w[(round+1)*4] && ws[1] == w[(round+1)*4+1] && ws[2] == w[(round+1)*4+2] && ws[3] == w[(round+1)*4+3];
//    assert xmm1 == seq_to_Quadword(ws);
    AESNI_imc(xmm1, xmm1);
    Store128_buffer(heap0, rdx, xmm1, 16*(round+1), k_b, round+1);

//    dw_out := dw_in + Quadword_to_seq(xmm1);
//    lemma_KeyInversionRoundHelper(round+1, key, w, dw_in, dw_out);
//
//    forall(j) round+1 < j <= 10 implies heap0[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]) by {
//    }
//
//    forall(j) 0 <= j <= round + 1 implies heap0[k_b].quads[rdx + 16*j].v == Quadword(dw_out[4*j], dw_out[4*j+1], dw_out[4*j+2], dw_out[4*j+3]) by {
//    }
}

procedure KeyInversionRoundUnrolledRecursive(
    inline rounds:int,
//    inline taint:taint,
    ghost k_b:buffer128)
//    ghost key:aes_key_LE(AES_128),
//    ghost w:seq(nat32),
//    ghost dw_in:seq(nat32)
//    ) returns (
//    ghost dw_out:seq(nat32)
//    )
    {:recursive}
    requires/ensures
        validSrcAddrs128(heap0, rdx, k_b, 11);
    requires
        0 <= rounds <= 9;
        rdx % 16 == 0;
//        SeqLength(w) == 44;
//        SeqLength(dw_in) == 4;
//        KeyExpansionPredicate(key, AES_128, w);
//        EqInvkey_expansion_partial(key, AES_128, dw_in, 0);
//        heap0[k_b].quads[rdx+16*0].v == Quadword(dw_in[4*0], dw_in[4*0+1], dw_in[4*0+2], dw_in[4*0+3]);
//        forall(j) 0 <= j <= 0 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(dw_in[4*j], dw_in[4*j+1], dw_in[4*j+2], dw_in[4*j+3]);
//        forall(j) 0 < j <= 10 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
    reads
        rdx;
    modifies
        heap0; xmm1; efl;
    ensures
        modifies_buffer128(k_b, old(heap0), heap0);
//        forall(a) (a < rdx || a >= rdx + 176) && old(heap0)[k_b].quads?[a] ==> heap0[k_b].quads?[a] && heap0[k_b].quads[a] == old(heap0)[k_b].quads[a];
//        SeqLength(dw_out) == 4*(rounds+1);
//        forall(j) 0 <= j <= rounds ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(dw_out[4*j], dw_out[4*j+1], dw_out[4*j+2], dw_out[4*j+3]);
//        forall(j) rounds < j <= 10 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
//        EqInvkey_expansion_partial(key, AES_128, dw_out, rounds);
{
//    inline if (0 < rounds <= 9) {
//        let dw_mid := KeyInversionRoundUnrolledRecursive(rounds-1, taint, k_b, key, w, dw_in);
//        dw_out := KeyInversionRound(rounds-1, taint, k_b, key, w, dw_mid);
//    }
//    else {
//        dw_out := dw_in;
//    }
}

procedure KeyInversionImpl(
//    ghost key:aes_key_LE(AES_128),
//    ghost w:seq(nat32),
//    inline taint:taint,
    ghost k_b:buffer128)
//    ) returns (
//    ghost dw:seq(nat32)
//    )
    requires/ensures
        validSrcAddrs128(heap0, rdx, k_b, 11);
    requires
        rdx % 16 == 0;
//        SeqLength(w) == 44;
//        forall(j) 0 <= j <= 10 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
//        KeyExpansionPredicate(key, AES_128, w);
    ensures
        modifies_buffer128(k_b, old(heap0), heap0);
//        forall(a) (a < rdx || a >= rdx + 176) && old(heap0)[k_b].quads?[a] ==> heap0[k_b].quads?[a] && heap0[k_b].quads[a] == old(heap0)[k_b].quads[a];
//        SeqLength(dw) == 44;
//        EqInvKeyExpansionPredicate(key, AES_128, dw);
//        forall(j) 0 <= j <= 10 ==> heap0[k_b].quads[rdx + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]);
    reads
        rdx;
    modifies
        heap0; xmm1; efl;
{
//    lemma_KeyExpansionPredicateImpliesExpandKey(key, AES_128, w);
//    let dw1 := seq(w[0], w[1], w[2], w[3]);
//
//    let dw2 := KeyInversionRoundUnrolledRecursive(9, taint, k_b, key, w, dw1);
    KeyInversionRoundUnrolledRecursive(9, k_b);
//
//    dw := dw2 + seq(w[40], w[41], w[42], w[43]);
//    assert SeqLength(dw) == 44;
//    forall(j) 0 <= j <= 10 implies heap0[k_b].quads[rdx + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]) by
//    {
//    }
//    assert EqInvKeyExpansionPredicate(key, AES_128, dw);
}

procedure KeyExpansionAndInversionStdcall(
//    inline taint:taint,
    inline win:bool,
    ghost input_key_b:buffer128,
    ghost output_key_expansion_b:buffer128)
//    ) returns (
//    ghost dw:seq(nat32)
//    )
    reads
        rcx; rsi; rdi;
    modifies
        rdx;
        heap0; xmm1; xmm2; xmm3; efl;
    requires
//        HasStackSlots(stack, 2);
        let key_ptr := if win then rcx else rdi;
        let key_expansion_ptr := if win then rdx else rsi;
        key_ptr % 16 == 0;
        key_expansion_ptr % 16 == 0;
        validSrcAddrs128(heap0, key_ptr, input_key_b, 1);
        validDstAddrs128(heap0, key_expansion_ptr, output_key_expansion_b, 11);
    ensures
        let key_ptr := if win then rcx else rdi;
        let key_expansion_ptr := if win then rdx else rsi;
//        let key := Quadword_to_seq(old(heap0)[input_key_id].quads[key_ptr].v);
//        SeqLength(dw) == 44;
//        ValidSrcAddrs(heap0, output_key_expansion_id, key_expansion_ptr, 128, taint, 176);
        validSrcAddrs128(heap0, key_expansion_ptr, output_key_expansion_b, 11);
        modifies_buffer128(output_key_expansion_b, old(heap0), heap0);
//        (forall(a) (a < key_expansion_ptr || a >= key_expansion_ptr + 176) && old(heap0)[output_key_expansion_id].quads?[a] ==> heap0[output_key_expansion_id].quads?[a] && heap0[output_key_expansion_id].quads[a] == old(heap0)[output_key_expansion_id].quads[a]);
//        (forall(j) 0 <= j <= 10 ==> heap0[output_key_expansion_id].quads[key_expansion_ptr + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]));
//        EqInvKeyExpansionPredicate(key, AES_128, dw);
{
    let key_ptr := if win then rcx else rdi;
    let key_expansion_ptr := if win then rdx else rsi;
//    let key := Quadword_to_seq(heap0[input_key_id].quads[key_ptr].v);

//    LoadStack(eax, 0);                                     // eax := key_ptr (from stack position 0)
    inline if (win)
    {
        Load128_buffer(heap0, xmm1, rcx, 0, input_key_b, 0);
    }
    else
    {
        Load128_buffer(heap0, xmm1, rdi, 0, input_key_b, 0);
        Mov64(rdx, rsi);
    }
//    let w := KeyExpansionImpl(key, taint, output_key_expansion_id); // expand key from xmm1 to region pointed to by rdx
//    dw := KeyInversionImpl(key, w, taint, output_key_expansion_id);
    let key := quad32_to_seq(xmm1);
    KeyExpansionImpl(key, output_key_expansion_b); // expand key from xmm1 to region pointed to by rdx

//    forall(j) 0 <= j <= 10 implies heap0[output_key_expansion_id].quads[key_expansion_ptr + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]) by
//    {
//        assert heap0[output_key_expansion_id].quads[rdx + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]);
//        assert rdx == key_expansion_ptr;
//    }

    // Clear secrets out of registers
//    Xor32(eax, eax);
    Pxor(xmm1, xmm1);
    Pxor(xmm2, xmm2);
    Pxor(xmm3, xmm3);
}
*/
