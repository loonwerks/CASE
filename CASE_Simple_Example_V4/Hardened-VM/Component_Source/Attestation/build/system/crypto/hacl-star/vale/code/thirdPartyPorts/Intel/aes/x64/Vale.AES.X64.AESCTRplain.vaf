include "../../../../arch/x64/Vale.X64.InsBasic.vaf"
include "../../../../arch/x64/Vale.X64.InsMem.vaf"
include "../../../../arch/x64/Vale.X64.InsVector.vaf"
include "../../../../arch/x64/Vale.X64.InsAes.vaf"
include "Vale.AES.X64.AESCTR.vaf"
include{:fstar}{:open} "Vale.Def.Prop_s"
include{:fstar}{:open} "Vale.Def.Opaque_s"
include{:fstar}{:open} "Vale.Def.Words_s"
include{:fstar}{:open} "Vale.Def.Types_s"
include{:fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "Vale.AES.AES_s"
include{:fstar}{:open} "Vale.X64.Machine_s"
include{:fstar}{:open} "Vale.X64.Memory"
include{:fstar}{:open} "Vale.X64.State"
include{:fstar}{:open} "Vale.X64.Decls"
include{:fstar}{:open} "Vale.X64.QuickCode"
include{:fstar}{:open} "Vale.X64.QuickCodes"
include{:fstar}{:open} "Vale.Arch.Types"
include{:fstar}{:open} "Vale.AES.AES_helpers"
include{:fstar}{:open} "Vale.Poly1305.Math"
include{:fstar}{:open} "Vale.AES.GCTR_s"
include{:fstar}{:open} "Vale.AES.GCTR"
include{:fstar}{:open} "Vale.Arch.TypesNative"
include{:fstar}{:open} "Vale.X64.CPU_Features_s"

module Vale.AES.X64.AESCTRplain

#verbatim{:interface}{:implementation}
open Vale.Def.Prop_s
open Vale.Def.Opaque_s
open Vale.Def.Words_s
open Vale.Def.Types_s
open FStar.Seq
open Vale.AES.AES_s
open Vale.X64.Machine_s
open Vale.X64.Memory
open Vale.X64.State
open Vale.X64.Decls
open Vale.X64.InsBasic
open Vale.X64.InsMem
open Vale.X64.InsVector
open Vale.X64.InsAes
open Vale.X64.QuickCode
open Vale.X64.QuickCodes
open Vale.Arch.Types
open Vale.AES.AES_helpers
open Vale.Poly1305.Math    // For lemma_poly_bits64()
open Vale.AES.GCTR_s
open Vale.AES.GCTR
open Vale.Arch.TypesNative
open Vale.AES.X64.AESCTR
open Vale.X64.CPU_Features_s
#endverbatim

// Intel's LOOP_4
procedure Aes_ctr_loop_body(
        inline alg:algorithm,  // Intel simply passes number of rounds (nr) as a dynamic parameter.  Saves code space but adds extra instructions to the fast path.  Maybe branch predictor is good enough for it not to matter
        ghost old_plain_ptr:nat64,
        ghost old_out_ptr:nat64,
        ghost old_num_quad_blocks:nat64,
        ghost count:nat,
        ghost plain_b:buffer128,
        ghost out_b:buffer128,

        ghost key:seq(nat32),
        ghost round_keys:seq(quad32),
        ghost keys_b:buffer128,

        ghost icb_BE:quad32,
        ghost iv:quad32)
    {:quick}
    {:options z3rlimit(400)}
    lets
        plain_ptr @= r9; num_quad_blocks @= rdx; out_ptr @= r10; key_ptr @= r8; iv_lower64 @= rdi;
        const2_1 @= xmm9; const4_3 @= xmm10; mask64 @= xmm0; four @= xmm15;
        rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
        tmp_xmm @= xmm7;
    requires
        sse_enabled;

        // There's at least one block of four left to do
        0 < num_quad_blocks <= old_num_quad_blocks;

        // We've already done count blocks
        count == old_num_quad_blocks - num_quad_blocks;

        // Valid ptrs and buffers
        validSrcAddrs128(heap0, old_plain_ptr,  plain_b, old_num_quad_blocks * 4, memLayout, Secret);
        validDstAddrs128(heap1, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memLayout, Secret);
        plain_ptr == old_plain_ptr + count * 64;
        out_ptr == old_out_ptr + count * 64;
        old_plain_ptr + old_num_quad_blocks * 64 < pow2_64;
        old_out_ptr + old_num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);

        // XMM constants are correct
        mask64 == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
        four == Mkfour(4, 0, 4, 0);

        // Counters are correct
        4*count < pow2_32 - 4;  // Simplifies the statement of add_wrap32 in the postcondition below
        const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 1))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 0))));
        const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 3))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 2))));

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        // GCM reqs
        iv == reverse_bytes_quad32(icb_BE);
        iv_lower64 == lo64(iv);

        // GCTR progress
        gctr_partial_def(alg, 4*count, buffer128_as_seq(heap0, plain_b), buffer128_as_seq(heap1, out_b), key, icb_BE);

    reads key_ptr; iv_lower64; mask64; four; memLayout; heap0;
    modifies const2_1; const4_3; rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
             tmp_xmm; plain_ptr; num_quad_blocks; out_ptr; r12; efl; heap1;
    ensures
        // Framing
        modifies_buffer128(out_b, old(heap1), heap1);

        // Valid ptrs and buffers
        validSrcAddrs128(heap0, old_plain_ptr,  plain_b, old_num_quad_blocks * 4, memLayout, Secret);
        validDstAddrs128(heap1, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memLayout, Secret);
        plain_ptr == old(plain_ptr) + 64;
        out_ptr   == old(out_ptr)   + 64;
        num_quad_blocks == old(num_quad_blocks) - 1;

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        // Counters are incremented
        const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 1))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 0))));
        const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 3))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 2))));

        // GCTR progress
        gctr_partial_def(alg, 4*count+4, buffer128_as_seq(heap0, plain_b), buffer128_as_seq(heap1, out_b), key, icb_BE);
{
    lemma_insrq_extrq_relations(ctr1, iv);
    lemma_insrq_extrq_relations(ctr2, iv);
    lemma_insrq_extrq_relations(ctr3, iv);
    lemma_insrq_extrq_relations(ctr4, iv);
    Pinsrq(ctr1, iv_lower64, 0);  // ctr1==12==13==14 == LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | * | *
    Pinsrq(ctr2, iv_lower64, 0);
    Pinsrq(ctr3, iv_lower64, 0);
    Pinsrq(ctr4, iv_lower64, 0);

    Shufpd(ctr1, const2_1, 2); // ctr1 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count)
    Shufpd(ctr2, const2_1, 0); // ctr2 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+1)
    Shufpd(ctr3, const4_3, 2); // ctr3 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+2)
    Shufpd(ctr4, const4_3, 0); // ctr4 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+3)

    reveal_reverse_bytes_quad32(icb_BE);
    reveal_reverse_bytes_quad32(ctr1);
    reveal_reverse_bytes_quad32(ctr2);
    reveal_reverse_bytes_quad32(ctr3);
    reveal_reverse_bytes_quad32(ctr4);

    Pshufb64(const2_1, mask64);     // const2_1 = LSB: 2 | bswap(ivec_hi) | 1 | bswap(ivec_hi)
    Pshufb64(const4_3, mask64);     // const4_3 = LSB: 4 | bswap(ivec_hi) | 3 | bswap(ivec_hi)

    // Load the next four round key blocks
    Load128_buffer(heap0, rk1,  key_ptr,  0, Secret, keys_b, 0);
    Load128_buffer(heap0, rk2,  key_ptr, 16, Secret, keys_b, 1);
    Load128_buffer(heap0, rk3, key_ptr, 32, Secret, keys_b, 2);
    Load128_buffer(heap0, rk4,  key_ptr, 48, Secret, keys_b, 3);
    assert rk1  == index(round_keys, 0);
    assert rk2  == index(round_keys, 1);
    assert rk3 == index(round_keys, 2);
    assert rk4  == index(round_keys, 3);

    // Pre-emptively increment our counters
    Paddd(const2_1, four);        // const2_1 = LSB: 6 | bswap(ivec_hi) | 5 | bswap(ivec_hi)
    Paddd(const4_3, four);        // const4_3 = LSB: 8 | bswap(ivec_hi) | 7 | bswap(ivec_hi)

    // Begin AES block encrypt by xor'ing four blocks of counters with the key block 0
    // At this point, we're computing aes_encrypt_LE of bswap(inc32(icb_BE, 4*count + 0..3)) == aes_encrypt_BE of inc32(icb_BE, 4*count + 0..3)
    let in1 := ctr1;
    let in2 := ctr2;
    let in3 := ctr3;
    let in4 := ctr4;

    Pxor(ctr1, rk1);
    Pxor(ctr2, rk1);
    Pxor(ctr3, rk1);
    Pxor(ctr4, rk1);

    Pshufb64(const2_1, mask64);      // const2_1 = LSB: ivec_hi | bswap(6) | ivec_hi | bswap(5)
    Pshufb64(const4_3, mask64);      // const4_3 = LSB: ivec_hi | bswap(8) | ivec_hi | bswap(7)

    Aes_ctr_encrypt(alg, key, round_keys, keys_b, in1, in2, in3, in4);

    // Xor the plaintext with the encrypted counter
    // TODO: Intel does this using XMM operands
    Load128_buffer(heap0, tmp_xmm, plain_ptr,  0, Secret, plain_b, 4*count + 0);
    Pxor(ctr1, tmp_xmm);
    Load128_buffer(heap0, tmp_xmm, plain_ptr, 16, Secret, plain_b, 4*count + 1);
    Pxor(ctr2, tmp_xmm);
    Load128_buffer(heap0, tmp_xmm, plain_ptr, 32, Secret, plain_b, 4*count + 2);
    Pxor(ctr3, tmp_xmm);
    Load128_buffer(heap0, tmp_xmm, plain_ptr, 48, Secret, plain_b, 4*count + 3);
    Pxor(ctr4, tmp_xmm);

    // Store the cipher text in output
    Store128_buffer(heap1, out_ptr, ctr1,  0, Secret, out_b, 4*count + 0);
    Store128_buffer(heap1, out_ptr, ctr2, 16, Secret, out_b, 4*count + 1);
    Store128_buffer(heap1, out_ptr, ctr3, 32, Secret, out_b, 4*count + 2);
    Store128_buffer(heap1, out_ptr, ctr4, 48, Secret, out_b, 4*count + 3);

    lemma_quad32_xor_commutes_forall();
    //commute_sub_bytes_shift_rows_forall();

    assert slice(buffer128_as_seq(heap1, out_b), 0, 4 * count) == old(slice(buffer128_as_seq(heap1, out_b), 0, 4 * count));   // OBSERVE

    Sub64(rdx, 1);
    Add64(plain_ptr, 64);
    Add64(out_ptr, 64);
}

procedure Aes_counter_loop(
        inline alg:algorithm,
        ghost plain_b:buffer128,
        ghost out_b:buffer128,
        ghost key:seq(nat32),
        ghost round_keys:seq(quad32),
        ghost keys_b:buffer128)
    {:public}
    {:quick}
    {:options z3rlimit(400)}
    lets plain_ptr @= r9; num_quad_blocks @= rdx; out_ptr @= r10; key_ptr @= r8; iv_lower64 @= rdi;
         const2_1 @= xmm9; const4_3 @= xmm10; iv @= xmm7; mask @= xmm8; mask64 @= xmm0; four @= xmm15;
         rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
         ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES

    reads key_ptr; mask; memLayout; heap0;
    modifies mask64; four; iv; const2_1; const4_3; rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
             rax; plain_ptr; num_quad_blocks; out_ptr; iv_lower64; r12;
             efl; heap1;

    requires
        sse_enabled;

        // There's at least one block of four left to do
        0 < num_quad_blocks && 4*num_quad_blocks < pow2_32 - 4;

        // Valid ptrs and buffers
        validSrcAddrs128(heap0, plain_ptr,  plain_b, num_quad_blocks * 4, memLayout, Secret);
        validDstAddrs128(heap1, out_ptr,    out_b,   num_quad_blocks * 4, memLayout, Secret);
        plain_ptr + num_quad_blocks * 64 < pow2_64;
        out_ptr   + num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        // GCM reqs
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        modifies_buffer128(out_b, old(heap1), heap1);

        plain_ptr == old(plain_ptr) + 64 * old(num_quad_blocks);
        out_ptr   == old(out_ptr)   + 64 * old(num_quad_blocks);

        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        // GCTR
        gctr_partial_def(alg, 4*old(num_quad_blocks), buffer128_as_seq(heap0, plain_b), buffer128_as_seq(heap1, out_b), key, old(iv));
        iv == old(inc32(iv, 4*num_quad_blocks));
{
    // To interface with our existing code, start by reversing the IV
    Pshufb(iv, mask);

    // TODO: Optimize some of the shuffles below by copying iv before doing the shuffle above

    // Initialize the counters
    Mov128(const2_1, iv);

    InitPshufbDupMask(mask64, rax); // borrow the mask64 xmm for other masks
    PshufbDup(const2_1, mask64);    // const2_1 = LSB: r(iv.hi3) | r(iv.hi2) | r(iv.hi3) | r(iv.h2)
    Mov128(const4_3, const2_1);
    ZeroXmm(rk1);
    PinsrdImm(rk1, 1, 2, rax);  // rk1 = LSB: 0 | 0 | 1 | 0
    Paddd(const2_1, rk1);       // const2_1 = LSB: r(iv.hi3) | r(iv.hi2) | r(iv.hi3) + 1 | r(iv.h2)
    assert const2_1 == Mkfour(reverse_bytes_nat32(iv.hi3), reverse_bytes_nat32(iv.hi2), add_wrap32(reverse_bytes_nat32(iv.hi3), 1), reverse_bytes_nat32(iv.hi2));

    PinsrdImm(rk1, 3, 2, rax);
    PinsrdImm(rk1, 2, 0, rax);  // rk1 = LSB: 2 | 0 | 3 | 0
    Paddd(const4_3, rk1);       // const4_3 = LSB: r(iv.hi3) + 2 | r(iv.hi2) | r(iv.hi3) + 3 | r(iv.h2)

    reveal_reverse_bytes_quad32(const2_1);
    Pshufb(const2_1, mask);       // const2_1 = LSB: iv.hi2 | r( r(iv.hi3) + 1) | iv.hi2 | iv.hi3

    reveal_reverse_bytes_quad32(const4_3);
    Pshufb(const4_3, mask);       // const4_3 = LSB: iv.hi2 | r( r(iv.hi3) + 3) | iv.hi2 | r( r(iv.hi3) + 2)

    reveal_reverse_bytes_quad32(iv);
    reveal_reverse_bytes_quad32(reverse_bytes_quad32(iv));

    // Store rest of iv in a 64-bit register
    Pextrq(iv_lower64, iv, 0);

    // Create the XMM constants
    InitPshufb64Mask(mask64, rax);

    ZeroXmm(four);
    PinsrdImm(four, 4, 0, rax);
    PinsrdImm(four, 4, 2, rax);

    ghost var count:nat := 0;
    let icb_BE:quad32 := reverse_bytes_quad32(iv);
    let old_iv:quad32 := iv;

    while (num_quad_blocks > 0)
        invariant
            sse_enabled;

            0 < old(num_quad_blocks) && 4*old(num_quad_blocks) < pow2_32 - 4;
            0 <= num_quad_blocks <= old(num_quad_blocks);
            count == old(num_quad_blocks) - num_quad_blocks;
            0 <= count <= pow2_32 - 4;

            // Framing
            modifies_buffer128(out_b, old(heap1), heap1);

            // Valid ptrs and buffers
            validSrcAddrs128(heap0, old(plain_ptr),  plain_b, old(num_quad_blocks * 4), memLayout, Secret);
            validDstAddrs128(heap1, old(out_ptr),    out_b,   old(num_quad_blocks * 4), memLayout, Secret);
            plain_ptr == old(plain_ptr) + count * 64;
            out_ptr == old(out_ptr) + count * 64;
            old(plain_ptr) + old(num_quad_blocks) * 64 < pow2_64;
            old(out_ptr) + old(num_quad_blocks) * 64 < pow2_64;
            buffer_length(plain_b)  <= buffer_length(out_b);

            // XMM constants are correct
            mask64 == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
            four == Mkfour(4, 0, 4, 0);

            // Counters are correct
            num_quad_blocks > 0 ==> 4*count < pow2_32 - 4;
            num_quad_blocks <= 0 ==> 4*count < pow2_32;
            const2_1 == Mkfour(old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 1))), old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 0))));
            const4_3 == Mkfour(old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 3))), old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 2))));

            // AES reqs
            aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

            // GCM reqs
            old_iv == reverse_bytes_quad32(icb_BE);
            iv_lower64 == lo64(old_iv);

            // GCTR progress
            length(slice(buffer128_as_seq(heap1, out_b), 0, 4 * old(num_quad_blocks))) > 0;
            gctr_partial_def(alg, 4*count, buffer128_as_seq(heap0, plain_b), buffer128_as_seq(heap1, out_b), key, icb_BE);
        decreases num_quad_blocks;
    {
        Aes_ctr_loop_body(alg, old(plain_ptr), old(out_ptr), old(num_quad_blocks), count, plain_b, out_b, key, round_keys, keys_b, icb_BE, old_iv);
        count := count + 1;
    }

    Mov128(iv, const2_1);
    lemma_insrq_extrq_relations(iv, old_iv);
    Pinsrq(iv, iv_lower64, 0);
    //assert iv == Mkfour(old_iv.lo0, old_iv.lo1, old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, 4*old(num_quad_blocks))));
    // Reversed: iv == Mkfour(r (r (add_wrap32(r(old_iv.hi3), 4*old(num_quad_blocks)))), r(old_iv.hi2), r(old_iv.lo1), r(old_iv.lo0));
    //              == Mkfour( add_wrap32(old(iv.lo0, 4*old(num_quad_blocks))), old(iv.lo2), old(iv.hi2), old(iv).hi3));
    //              == inc32(old(iv), 4*old(num_quad_blocks))
    reveal_reverse_bytes_quad32(iv);
    Pshufb(iv, mask);
    //assert iv == old(inc32(iv, 4*num_quad_blocks));
}
