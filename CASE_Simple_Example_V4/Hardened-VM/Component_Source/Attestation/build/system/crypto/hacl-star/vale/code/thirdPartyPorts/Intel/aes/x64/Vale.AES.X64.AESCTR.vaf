include "../../../../arch/x64/Vale.X64.InsBasic.vaf"
include "../../../../arch/x64/Vale.X64.InsMem.vaf"
include "../../../../arch/x64/Vale.X64.InsVector.vaf"
include "../../../../arch/x64/Vale.X64.InsAes.vaf"
include "../../../../crypto/aes/x64/Vale.AES.X64.GHash.vaf"
include{:fstar}{:open} "Vale.Def.Prop_s"
include{:fstar}{:open} "Vale.Def.Opaque_s"
include{:fstar}{:open} "Vale.Def.Words_s"
include{:fstar}{:open} "Vale.Def.Types_s"
include{:fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "Vale.AES.AES_s"
include{:fstar}{:open} "Vale.X64.Machine_s"
include{:fstar}{:open} "Vale.X64.Memory"
include{:fstar}{:open} "Vale.X64.State"
include{:fstar}{:open} "Vale.X64.Decls"
include{:fstar}{:open} "Vale.X64.QuickCode"
include{:fstar}{:open} "Vale.X64.QuickCodes"
include{:fstar}{:open} "Vale.Arch.Types"
include{:fstar}{:open} "Vale.AES.AES_helpers"
include{:fstar}{:open} "Vale.Poly1305.Math"
include{:fstar}{:open} "Vale.AES.GCM_helpers"
include{:fstar}{:open} "Vale.AES.GCTR_s"
include{:fstar}{:open} "Vale.AES.GCTR"
include{:fstar}{:open} "Vale.Arch.TypesNative"
include{:fstar}{:open} "Vale.AES.GHash"
include{:fstar}{:open} "Vale.X64.CPU_Features_s"

module Vale.AES.X64.AESCTR

#verbatim{:interface}{:implementation}
open Vale.Def.Prop_s
open Vale.Def.Opaque_s
open Vale.Def.Words_s
open Vale.Def.Types_s
open FStar.Seq
open Vale.Arch.Types
open Vale.Arch.HeapImpl
open Vale.AES.AES_s
open Vale.X64.Machine_s
open Vale.X64.Memory
open Vale.X64.State
open Vale.X64.Decls
open Vale.X64.InsBasic
open Vale.X64.InsMem
open Vale.X64.InsVector
open Vale.X64.InsAes
open Vale.X64.QuickCode
open Vale.X64.QuickCodes
open Vale.AES.AES_helpers
open Vale.Poly1305.Math    // For lemma_poly_bits64()
open Vale.AES.GCM_helpers
open Vale.AES.GCTR_s
open Vale.AES.GCTR
open Vale.Arch.TypesNative
open Vale.AES.X64.GHash
open Vale.AES.GHash
open Vale.X64.CPU_Features_s
#endverbatim

#verbatim{:interface}
let aes_reqs
  (alg:algorithm) (key:seq nat32) (round_keys:seq quad32) (keys_b:buffer128)
  (key_ptr:nat64) (heap0:vale_heap) (layout:vale_heap_layout) : prop0
  =
  aesni_enabled /\ avx_enabled /\
  (alg = AES_128 || alg = AES_256) /\
  is_aes_key_LE alg key /\
  length(round_keys) == nr(alg) + 1 /\
  round_keys == key_to_round_keys_LE alg key /\
  key_ptr == buffer_addr keys_b heap0 /\
  validSrcAddrs128 heap0 key_ptr keys_b (nr alg + 1) layout Secret /\
  buffer128_as_seq heap0 keys_b == round_keys
#endverbatim
function aes_reqs(alg:algorithm, key:seq(nat32), round_keys:seq(quad32), keys_b:buffer128,
    key_ptr:nat64, heap0:vale_heap, layout:vale_heap_layout) : prop extern;

ghost procedure finish_aes_encrypt_le(ghost alg:algorithm, ghost input_LE:quad32, ghost key:seq(nat32))
    requires
        is_aes_key_LE(alg, key);
    ensures
        aes_encrypt_LE(alg, key, input_LE) == eval_cipher(alg, input_LE, key_to_round_keys_LE(alg, key));
{
    aes_encrypt_LE_reveal();
    eval_cipher_reveal();
}

// TODO: Write one procedure that unrolls to these variants?
// These procedures help walk F* through the recursive definition of rounds
procedure Aes_round_4way(
        ghost alg:algorithm,
        ghost n:pos,
        in round_key:xmm,
        ghost init1:quad32,
        ghost init2:quad32,
        ghost init3:quad32,
        ghost init4:quad32,
        ghost round_keys:seq(quad32))
    {:quick exportOnly}
    lets
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
    modifies
        ctr1; ctr2; ctr3; ctr4;
        efl;
    requires
        aesni_enabled;
        1 <= n < nr(alg) <= length(round_keys);
        ctr1 == eval_rounds(init1, round_keys, n - 1);
        ctr2 == eval_rounds(init2, round_keys, n - 1);
        ctr3 == eval_rounds(init3, round_keys, n - 1);
        ctr4 == eval_rounds(init4, round_keys, n - 1);
        round_key == index(round_keys, n);
         2 != @round_key;
        12 != @round_key;
        13 != @round_key;
        14 != @round_key;
    ensures
        ctr1 == eval_rounds(init1, round_keys, n);
        ctr2 == eval_rounds(init2, round_keys, n);
        ctr3 == eval_rounds(init3, round_keys, n);
        ctr4 == eval_rounds(init4, round_keys, n);
{
    eval_rounds_reveal();
    commute_sub_bytes_shift_rows_forall();
    AESNI_enc(ctr1, round_key);
    AESNI_enc(ctr2, round_key);
    AESNI_enc(ctr3, round_key);
    AESNI_enc(ctr4, round_key);
}

procedure Aes_2rounds_4way(
        ghost alg:algorithm,
        ghost n:pos,
        ghost init1:quad32,
        ghost init2:quad32,
        ghost init3:quad32,
        ghost init4:quad32,
        ghost round_keys:seq(quad32))
    {:quick}
    lets
        rk1 @= xmm3; rk2 @= xmm4;                                   // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
    reads
        rk1; rk2;
    modifies
        ctr1; ctr2; ctr3; ctr4;
        efl;
    requires
        aesni_enabled;
        1 <= n < nr(alg) - 1 && nr(alg) <= length(round_keys);
        ctr1 == eval_rounds(init1, round_keys, n - 1);
        ctr2 == eval_rounds(init2, round_keys, n - 1);
        ctr3 == eval_rounds(init3, round_keys, n - 1);
        ctr4 == eval_rounds(init4, round_keys, n - 1);
        rk1  == index(round_keys, n);
        rk2  == index(round_keys, n + 1);
    ensures
        ctr1 == eval_rounds(init1, round_keys, n + 1);
        ctr2 == eval_rounds(init2, round_keys, n + 1);
        ctr3 == eval_rounds(init3, round_keys, n + 1);
        ctr4 == eval_rounds(init4, round_keys, n + 1);
{
    Aes_round_4way(alg, n,     rk1,  init1, init2, init3, init4, round_keys);
    Aes_round_4way(alg, n + 1, rk2,  init1, init2, init3, init4, round_keys);
}

procedure Aes_3rounds_4way(
        ghost alg:algorithm,
        ghost n:pos,
        ghost init1:quad32,
        ghost init2:quad32,
        ghost init3:quad32,
        ghost init4:quad32,
        ghost round_keys:seq(quad32))
    {:quick}
    lets
        rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;                     // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
    reads
        rk2; rk3; rk4;
    modifies
        ctr1; ctr2; ctr3; ctr4;
        efl;
    requires
        aesni_enabled;
        1 <= n < nr(alg) - 2 && nr(alg) <= length(round_keys);
        ctr1 == eval_rounds(init1, round_keys, n - 1);
        ctr2 == eval_rounds(init2, round_keys, n - 1);
        ctr3 == eval_rounds(init3, round_keys, n - 1);
        ctr4 == eval_rounds(init4, round_keys, n - 1);
        rk2  == index(round_keys, n + 0);
        rk3 == index(round_keys, n + 1);
        rk4  == index(round_keys, n + 2);
    ensures
        ctr1 == eval_rounds(init1, round_keys, n + 2);
        ctr2 == eval_rounds(init2, round_keys, n + 2);
        ctr3 == eval_rounds(init3, round_keys, n + 2);
        ctr4 == eval_rounds(init4, round_keys, n + 2);
{
    Aes_round_4way(alg, n + 0, rk2,  init1, init2, init3, init4, round_keys);
    Aes_round_4way(alg, n + 1, rk3, init1, init2, init3, init4, round_keys);
    Aes_round_4way(alg, n + 2, rk4,  init1, init2, init3, init4, round_keys);
}

procedure Aes_4rounds_4way(
        ghost alg:algorithm,
        ghost n:pos,
        ghost init1:quad32,
        ghost init2:quad32,
        ghost init3:quad32,
        ghost init4:quad32,
        ghost round_keys:seq(quad32))
    {:quick}
    lets
        rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
    reads
        rk1; rk2; rk3; rk4;
    modifies
        ctr1; ctr2; ctr3; ctr4;
        efl;
    requires
        aesni_enabled;
        1 <= n < nr(alg) - 3 && nr(alg) <= length(round_keys);
        ctr1 == eval_rounds(init1, round_keys, n - 1);
        ctr2 == eval_rounds(init2, round_keys, n - 1);
        ctr3 == eval_rounds(init3, round_keys, n - 1);
        ctr4 == eval_rounds(init4, round_keys, n - 1);
        rk1  == index(round_keys, n);
        rk2  == index(round_keys, n + 1);
        rk3 == index(round_keys, n + 2);
        rk4  == index(round_keys, n + 3);
    ensures
        ctr1 == eval_rounds(init1, round_keys, n + 3);
        ctr2 == eval_rounds(init2, round_keys, n + 3);
        ctr3 == eval_rounds(init3, round_keys, n + 3);
        ctr4 == eval_rounds(init4, round_keys, n + 3);
{
    Aes_round_4way(alg, n,     rk1,  init1, init2, init3, init4, round_keys);
    Aes_round_4way(alg, n + 1, rk2,  init1, init2, init3, init4, round_keys);
    Aes_round_4way(alg, n + 2, rk3, init1, init2, init3, init4, round_keys);
    Aes_round_4way(alg, n + 3, rk4,  init1, init2, init3, init4, round_keys);
}

procedure Aes_ctr_encrypt(
        inline alg:algorithm,  // Intel simply passes number of rounds (nr) as a dynamic parameter.  Saves code space but adds extra instructions to the fast path.  Maybe branch predictor is good enough for it not to matter
        ghost key:seq(nat32),
        ghost round_keys:seq(quad32),
        ghost keys_b:buffer128,
        ghost in1:quad32,
        ghost in2:quad32,
        ghost in3:quad32,
        ghost in4:quad32)
    {:public}
    {:quick}
    lets
        key_ptr @= r8;
        rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
    requires
        sse_enabled;

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);
        ctr1 == quad32_xor(in1, rk1);
        ctr2 == quad32_xor(in2, rk1);
        ctr3 == quad32_xor(in3, rk1);
        ctr4 == quad32_xor(in4, rk1);

        rk1 == index(round_keys, 0);
        rk2 == index(round_keys, 1);
        rk3 == index(round_keys, 2);
        rk4 == index(round_keys, 3);

    reads key_ptr; heap0; memLayout;
    modifies rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
             efl;
    ensures
        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        ctr1 == aes_encrypt_LE(alg, key, in1);
        ctr2 == aes_encrypt_LE(alg, key, in2);
        ctr3 == aes_encrypt_LE(alg, key, in3);
        ctr4 == aes_encrypt_LE(alg, key, in4);
{
    init_rounds_opaque(old(ctr1), round_keys);
    init_rounds_opaque(old(ctr2), round_keys);
    init_rounds_opaque(old(ctr3), round_keys);
    init_rounds_opaque(old(ctr4), round_keys);
    // Compute three AES rounds for all four counters
    Aes_3rounds_4way(alg, 1, old(ctr1), old(ctr2), old(ctr3), old(ctr4), round_keys);

    // Load the next four round key blocks
    Load128_buffer(heap0, rk1, key_ptr,  64, Secret, keys_b, 4);
    Load128_buffer(heap0, rk2, key_ptr,  80, Secret, keys_b, 5);
    Load128_buffer(heap0, rk3, key_ptr,  96, Secret, keys_b, 6);
    Load128_buffer(heap0, rk4, key_ptr, 112, Secret, keys_b, 7);
    assert rk1 == index(round_keys, 4);
    assert rk2 == index(round_keys, 5);
    assert rk3 == index(round_keys, 6);
    assert rk4 == index(round_keys, 7);

    // Do another 4 AES rounds for each of the four counters = 7 rounds total (not counting xor step)
    Aes_4rounds_4way(alg, 4, old(ctr1), old(ctr2), old(ctr3), old(ctr4), round_keys);

    // Load the next three round key blocks
    Load128_buffer(heap0, rk1, key_ptr, 128, Secret, keys_b, 8);
    Load128_buffer(heap0, rk2, key_ptr, 144, Secret, keys_b, 9);
    Load128_buffer(heap0, rk3, key_ptr, 160, Secret, keys_b, 10);
    assert rk1  == index(round_keys, 8);
    assert rk2  == index(round_keys, 9);
    assert rk3 == index(round_keys, 10);

    // Do another 2 AES rounds for each of the four counters = 9 rounds total (not counting xor step)
    Aes_2rounds_4way(alg, 8, old(ctr1), old(ctr2), old(ctr3), old(ctr4), round_keys);

    // TODO: AES_192 and AES_256 would do a few more rounds here
    inline if (alg = AES_256) {
        // Load 3 more round keys and shuffle the one in rk3 to rk1 (Intel does this by reloading from memory -- why?)
        Mov128(rk1, rk3);
        Load128_buffer(heap0, rk2, key_ptr, 176, Secret, keys_b, 11);
        Load128_buffer(heap0, rk3, key_ptr, 192, Secret, keys_b, 12);
        Load128_buffer(heap0, rk4, key_ptr, 208, Secret, keys_b, 13);
        assert rk1 == index(round_keys, 10);
        assert rk2 == index(round_keys, 11);
        assert rk3 == index(round_keys, 12);
        assert rk4 == index(round_keys, 13);

        // Do another 4 AES rounds for each of the four counters = 13 rounds total (not counting xor step)
        Aes_4rounds_4way(alg, 10, old(ctr1), old(ctr2), old(ctr3), old(ctr4), round_keys);

        // Load the final round key
        Load128_buffer(heap0, rk3, key_ptr, 224, Secret, keys_b, 14);
        assert rk3 == index(round_keys, 14);
    }

    // Intel's LAST_4
    //assert ctr1 == eval_rounds_def(quad32_xor(old_ctr1, index(round_keys, 0), round_keys, nr(alg) - 1));

    // Compute the last AES round for each of the four counters
    AESNI_enc_last(ctr1, rk3);
    AESNI_enc_last(ctr2, rk3);
    AESNI_enc_last(ctr3, rk3);
    AESNI_enc_last(ctr4, rk3);

    finish_cipher(alg, in1, round_keys);
    finish_cipher(alg, in2, round_keys);
    finish_cipher(alg, in3, round_keys);
    finish_cipher(alg, in4, round_keys);
    finish_aes_encrypt_le(alg, in1, key);
    finish_aes_encrypt_le(alg, in2, key);
    finish_aes_encrypt_le(alg, in3, key);
    finish_aes_encrypt_le(alg, in4, key);
}

procedure Aes_ctr_ghash(
        ghost old_out_ptr:nat64,
        ghost old_num_quad_blocks:nat64,
        ghost count:nat,
        ghost out_b:buffer128,
        ghost hash_init:quad32)
    {:quick}
    lets
        rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
        mask @= xmm8; hash @= xmm1; h @= xmm11;
    reads mask; h; ctr2; ctr3; ctr4; heap1; memLayout;
    modifies hash; rk4; rk1; rk2; rk3; ctr1; r12; efl;
    requires
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        0 <= count < old_num_quad_blocks;
        validDstAddrs128(heap1, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memLayout, Secret);

        // XMM constants are correct
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        // Partial GCM progress
        hash == ghash_incremental0(reverse_bytes_quad32(h), hash_init, slice(buffer128_as_seq(heap1, out_b), 0, 4 * count));

        ctr1 == index(buffer128_as_seq(heap1, out_b), 4*count+0);
        ctr2 == index(buffer128_as_seq(heap1, out_b), 4*count+1);
        ctr3 == index(buffer128_as_seq(heap1, out_b), 4*count+2);
        ctr4 == index(buffer128_as_seq(heap1, out_b), 4*count+3);

    ensures
        hash == ghash_incremental(reverse_bytes_quad32(h), hash_init, slice(buffer128_as_seq(heap1, out_b), 0, 4 * (count+1)));
{
    // Compute GHash on the four outputs
    let h_in_0 := ctr1;
    Compute_ghash_incremental_register();
    let hash0 := hash;

    Mov128(ctr1, ctr2);
    let h_in_1 := ctr1;
    Compute_ghash_incremental_register();
    let hash1 := hash;

    Mov128(ctr1, ctr3);
    let h_in_2 := ctr1;
    Compute_ghash_incremental_register();
    let hash2 := hash;

    Mov128(ctr1, ctr4);
    let h_in_3 := ctr1;
    Compute_ghash_incremental_register();
    lemma_ghash_registers(reverse_bytes_quad32(h), hash_init, old(hash), hash0, hash1, hash2, hash, h_in_0, h_in_1, h_in_2, h_in_3, buffer128_as_seq(heap1, out_b), 4*count);
}

// Intel's LOOP_4
procedure Aes_ctr_loop_body(
        inline alg:algorithm,  // Intel simply passes number of rounds (nr) as a dynamic parameter.  Saves code space but adds extra instructions to the fast path.  Maybe branch predictor is good enough for it not to matter
        ghost old_plain_ptr:nat64,
        ghost old_out_ptr:nat64,
        ghost old_num_quad_blocks:nat64,
        ghost count:nat,
        ghost plain_b:buffer128,
        ghost out_b:buffer128,

        ghost key:seq(nat32),
        ghost round_keys:seq(quad32),
        ghost keys_b:buffer128,

        ghost icb_BE:quad32,
        ghost iv:quad32,

        ghost hash_init:quad32)
    {:quick}
    {:options z3rlimit(800)}
    lets
        plain_ptr @= r9; num_quad_blocks @= rdx; out_ptr @= r10; key_ptr @= r8; iv_lower64 @= rdi;
        const2_1 @= xmm9; const4_3 @= xmm10; mask64 @= xmm0; four @= xmm15;
        rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
        tmp_xmm @= xmm7; mask @= xmm8; hash @= xmm1; h @= xmm11;
    reads key_ptr; iv_lower64; mask; mask64; four; memLayout; heap0; h;
    modifies
        const2_1; const4_3; rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
        tmp_xmm; hash; plain_ptr; num_quad_blocks; out_ptr; r12; efl; heap1;
    requires
        sse_enabled;

        // There's at least one block of four left to do
        0 < num_quad_blocks <= old_num_quad_blocks;

        // We've already done count blocks
        count == old_num_quad_blocks - num_quad_blocks;

        // Valid ptrs and buffers
        validSrcAddrs128(heap1, old_plain_ptr,  plain_b, old_num_quad_blocks * 4, memLayout, Secret);
        validDstAddrs128(heap1, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memLayout, Secret);
        plain_ptr == old_plain_ptr + count * 64;
        out_ptr == old_out_ptr + count * 64;
        old_plain_ptr + old_num_quad_blocks * 64 < pow2_64;
        old_out_ptr + old_num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);

        // XMM constants are correct
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
        mask64 == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
        four == Mkfour(4, 0, 4, 0);

        // Counters are correct
        4*count < pow2_32 - 4;  // Simplifies the statement of add_wrap32 in the postcondition below
        const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 1))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 0))));
        const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 3))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 2))));

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        // GCM reqs
        pclmulqdq_enabled;
        iv == reverse_bytes_quad32(icb_BE);
        iv_lower64 == lo64(iv);

        // GCTR progress
        gctr_partial_def(alg, 4*count, old(buffer128_as_seq(heap1, plain_b)), buffer128_as_seq(heap1, out_b), key, icb_BE);
        hash == ghash_incremental0(reverse_bytes_quad32(h), hash_init, slice(buffer128_as_seq(heap1, out_b), 0, 4 * count));

    ensures
        // Framing
        modifies_buffer128(out_b, old(heap1), heap1);

        // Valid ptrs and buffers
        validSrcAddrs128(heap1, old_plain_ptr,  plain_b, old_num_quad_blocks * 4, memLayout, Secret);
        validDstAddrs128(heap1, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memLayout, Secret);
        plain_ptr == old(plain_ptr) + 64;
        out_ptr   == old(out_ptr)   + 64;
        num_quad_blocks == old(num_quad_blocks) - 1;

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        // Counters are incremented
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
        const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 1))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 0))));
        const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 3))), iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count + 4), 2))));

        // GCTR progress
        gctr_partial_def(alg, 4*count+4, old(buffer128_as_seq(heap1, plain_b)), buffer128_as_seq(heap1, out_b), key, icb_BE);
        hash == ghash_incremental(reverse_bytes_quad32(h), hash_init, slice(buffer128_as_seq(heap1, out_b), 0, 4 * (count+1)));
{
    lemma_insrq_extrq_relations(ctr1, iv);
    lemma_insrq_extrq_relations(ctr2, iv);
    lemma_insrq_extrq_relations(ctr3, iv);
    lemma_insrq_extrq_relations(ctr4, iv);
    Pinsrq(ctr1, iv_lower64, 0);  // ctr1==12==13==14 == LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | * | *
    Pinsrq(ctr2, iv_lower64, 0);
    Pinsrq(ctr3, iv_lower64, 0);
    Pinsrq(ctr4, iv_lower64, 0);

    Shufpd(ctr1, const2_1, 2); // ctr1 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count)
    Shufpd(ctr2, const2_1, 0); // ctr2 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+1)
    Shufpd(ctr3, const4_3, 2); // ctr3 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+2)
    Shufpd(ctr4, const4_3, 0); // ctr4 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+3)

    reveal_reverse_bytes_quad32(icb_BE);
    reveal_reverse_bytes_quad32(ctr1);
    reveal_reverse_bytes_quad32(ctr2);
    reveal_reverse_bytes_quad32(ctr3);
    reveal_reverse_bytes_quad32(ctr4);

    Pshufb64(const2_1, mask64);     // const2_1 = LSB: 2 | bswap(ivec_hi) | 1 | bswap(ivec_hi)
    Pshufb64(const4_3, mask64);     // const4_3 = LSB: 4 | bswap(ivec_hi) | 3 | bswap(ivec_hi)

    // Load the next four round key blocks
    Load128_buffer(heap0, rk1,  key_ptr,  0, Secret, keys_b, 0);
    Load128_buffer(heap0, rk2,  key_ptr, 16, Secret, keys_b, 1);
    Load128_buffer(heap0, rk3, key_ptr, 32, Secret, keys_b, 2);
    Load128_buffer(heap0, rk4,  key_ptr, 48, Secret, keys_b, 3);
    assert rk1  == index(round_keys, 0);
    assert rk2  == index(round_keys, 1);
    assert rk3 == index(round_keys, 2);
    assert rk4  == index(round_keys, 3);

    // Pre-emptively increment our counters
    Paddd(const2_1, four);        // const2_1 = LSB: 6 | bswap(ivec_hi) | 5 | bswap(ivec_hi)
    Paddd(const4_3, four);        // const4_3 = LSB: 8 | bswap(ivec_hi) | 7 | bswap(ivec_hi)

    // Begin AES block encrypt by xor'ing four blocks of counters with the key block 0
    // At this point, we're computing aes_encrypt_LE of bswap(inc32(icb_BE, 4*count + 0..3)) == aes_encrypt_BE of inc32(icb_BE, 4*count + 0..3)
    let in1 := ctr1;
    let in2 := ctr2;
    let in3 := ctr3;
    let in4 := ctr4;

    Pxor(ctr1, rk1);
    Pxor(ctr2, rk1);
    Pxor(ctr3, rk1);
    Pxor(ctr4, rk1);

    Pshufb64(const2_1, mask64);      // const2_1 = LSB: ivec_hi | bswap(6) | ivec_hi | bswap(5)
    Pshufb64(const4_3, mask64);      // const4_3 = LSB: ivec_hi | bswap(8) | ivec_hi | bswap(7)

    Aes_ctr_encrypt(alg, key, round_keys, keys_b, in1, in2, in3, in4);

    // Xor the plaintext with the encrypted counter
    // TODO: Intel does this using XMM operands
    Load128_buffer(heap1, tmp_xmm, plain_ptr,  0, Secret, plain_b, 4*count + 0);
    Pxor(ctr1, tmp_xmm);
    Load128_buffer(heap1, tmp_xmm, plain_ptr, 16, Secret, plain_b, 4*count + 1);
    Pxor(ctr2, tmp_xmm);
    Load128_buffer(heap1, tmp_xmm, plain_ptr, 32, Secret, plain_b, 4*count + 2);
    Pxor(ctr3, tmp_xmm);
    Load128_buffer(heap1, tmp_xmm, plain_ptr, 48, Secret, plain_b, 4*count + 3);
    Pxor(ctr4, tmp_xmm);

    // Store the cipher text in output
    Store128_buffer(heap1, out_ptr, ctr1,  0, Secret, out_b, 4*count + 0);
    Store128_buffer(heap1, out_ptr, ctr2, 16, Secret, out_b, 4*count + 1);
    Store128_buffer(heap1, out_ptr, ctr3, 32, Secret, out_b, 4*count + 2);
    Store128_buffer(heap1, out_ptr, ctr4, 48, Secret, out_b, 4*count + 3);

    lemma_quad32_xor_commutes_forall();
    //commute_sub_bytes_shift_rows_forall();

    assert slice(buffer128_as_seq(heap1, out_b), 0, 4 * count) == old(slice(buffer128_as_seq(heap1, out_b), 0, 4 * count));   // OBSERVE

    // Compute GHash on the four outputs
    Aes_ctr_ghash(old_out_ptr, old_num_quad_blocks, count, out_b, hash_init);

    Sub64(rdx, 1);
    Add64(plain_ptr, 64);
    Add64(out_ptr, 64);
}

procedure Aes_counter_loop(
        inline alg:algorithm,
        ghost plain_b:buffer128,
        ghost out_b:buffer128,
        ghost key:seq(nat32),
        ghost round_keys:seq(quad32),
        ghost keys_b:buffer128)
    {:public}
    {:quick}
    {:options z3rlimit(800)}
    lets
        plain_ptr @= r9; num_quad_blocks @= rdx; out_ptr @= r10; key_ptr @= r8; iv_lower64 @= rdi;
        const2_1 @= xmm9; const4_3 @= xmm10; iv @= xmm7; mask64 @= xmm0; four @= xmm15;
        rk1 @= xmm3; rk2 @= xmm4; rk3 @= xmm5; rk4 @= xmm6;        // Round keys
        ctr1 @= xmm2; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
        mask @= xmm8; hash @= xmm1; h @= xmm11;

    reads mask; key_ptr; memLayout; heap0; h;
    modifies
        mask64; four; iv; const2_1; const4_3; rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
        rax; plain_ptr; num_quad_blocks; out_ptr; iv_lower64; r12; hash;
        efl; heap1;

    requires
        sse_enabled;

        // There's at least one block of four left to do
        0 < num_quad_blocks && 4*num_quad_blocks < pow2_32 - 4;

        // Valid ptrs and buffers
        validSrcAddrs128(heap1, plain_ptr,  plain_b, num_quad_blocks * 4, memLayout, Secret);
        validDstAddrs128(heap1, out_ptr,    out_b,   num_quad_blocks * 4, memLayout, Secret);
        plain_ptr + num_quad_blocks * 64 < pow2_64;
        out_ptr   + num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);

        buffers_disjoint128(plain_b, out_b);        // TODO: Remove this requirement

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

        // GCM reqs
        pclmulqdq_enabled;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        modifies_buffer128(out_b, old(heap1), heap1);
        validSrcAddrs128(heap1, old(out_ptr), out_b, old(num_quad_blocks) * 4, memLayout, Secret);

        plain_ptr == old(plain_ptr) + 64 * old(num_quad_blocks);
        out_ptr   == old(out_ptr)   + 64 * old(num_quad_blocks);

        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        // GCTR
        gctr_partial_def(alg, 4*old(num_quad_blocks), old(buffer128_as_seq(heap1, plain_b)), buffer128_as_seq(heap1, out_b), key, old(iv));
        iv == old(inc32(iv, 4*num_quad_blocks));

        // GHash
        length(slice(buffer128_as_seq(heap1, out_b), 0, 4 * old(num_quad_blocks))) > 0 /\
        hash == ghash_incremental(reverse_bytes_quad32(h), old(hash), slice(buffer128_as_seq(heap1, out_b), 0, 4 * old(num_quad_blocks)));
{
    // To interface with our existing code, start by reversing the IV
    Pshufb(iv, mask);

    // TODO: Optimize some of the shuffles below by copying iv before doing the shuffle above

    // Initialize the counters
    Mov128(const2_1, iv);

    InitPshufbDupMask(mask64, rax); // borrow the mask64 xmm for other masks
    PshufbDup(const2_1, mask64);    // const2_1 = LSB: r(iv.hi3) | r(iv.hi2) | r(iv.hi3) | r(iv.h2)
    Mov128(const4_3, const2_1);
    ZeroXmm(rk1);
    PinsrdImm(rk1, 1, 2, rax);  // rk1 = LSB: 0 | 0 | 1 | 0
    Paddd(const2_1, rk1);       // const2_1 = LSB: r(iv.hi3) | r(iv.hi2) | r(iv.hi3) + 1 | r(iv.h2)
    assert const2_1 == Mkfour(reverse_bytes_nat32(iv.hi3), reverse_bytes_nat32(iv.hi2), add_wrap32(reverse_bytes_nat32(iv.hi3), 1), reverse_bytes_nat32(iv.hi2));

    PinsrdImm(rk1, 3, 2, rax);
    PinsrdImm(rk1, 2, 0, rax);  // rk1 = LSB: 2 | 0 | 3 | 0
    Paddd(const4_3, rk1);       // const4_3 = LSB: r(iv.hi3) + 2 | r(iv.hi2) | r(iv.hi3) + 3 | r(iv.h2)

    reveal_reverse_bytes_quad32(const2_1);
    Pshufb(const2_1, mask);       // const2_1 = LSB: iv.hi2 | r( r(iv.hi3) + 1) | iv.hi2 | iv.hi3

    reveal_reverse_bytes_quad32(const4_3);
    Pshufb(const4_3, mask);       // const4_3 = LSB: iv.hi2 | r( r(iv.hi3) + 3) | iv.hi2 | r( r(iv.hi3) + 2)

    reveal_reverse_bytes_quad32(iv);
    reveal_reverse_bytes_quad32(reverse_bytes_quad32(iv));

    // Store rest of iv in a 64-bit register
    Pextrq(iv_lower64, iv, 0);

    // Create the XMM constants
    InitPshufb64Mask(mask64, rax);

    ZeroXmm(four);
    PinsrdImm(four, 4, 0, rax);
    PinsrdImm(four, 4, 2, rax);

    ghost var count:nat := 0;
    let icb_BE:quad32 := reverse_bytes_quad32(iv);
    let old_iv:quad32 := iv;

    while (num_quad_blocks > 0)
        invariant
            sse_enabled;

            0 < old(num_quad_blocks) && 4*old(num_quad_blocks) < pow2_32 - 4;
            0 <= num_quad_blocks <= old(num_quad_blocks);
            count == old(num_quad_blocks) - num_quad_blocks;
            0 <= count <= pow2_32 - 4;

            // Framing
            modifies_buffer128(out_b, old(heap1), heap1);
            buffer128_as_seq(heap1, plain_b) == old(buffer128_as_seq(heap1, plain_b));  // REVIEW: Shouldn't this follow from above + disjoint?

            // Valid ptrs and buffers
            validSrcAddrs128(heap1, old(plain_ptr),  plain_b, old(num_quad_blocks * 4), memLayout, Secret);
            validDstAddrs128(heap1, old(out_ptr),    out_b,   old(num_quad_blocks * 4), memLayout, Secret);
            plain_ptr == old(plain_ptr) + count * 64;
            out_ptr == old(out_ptr) + count * 64;
            old(plain_ptr) + old(num_quad_blocks) * 64 < pow2_64;
            old(out_ptr) + old(num_quad_blocks) * 64 < pow2_64;
            buffer_length(plain_b)  <= buffer_length(out_b);
            buffers_disjoint128(plain_b, out_b);        // TODO: Remove this requirement

            // XMM constants are correct
            mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
            mask64 == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
            four == Mkfour(4, 0, 4, 0);

            // Counters are correct
            num_quad_blocks > 0 ==> 4*count < pow2_32 - 4;
            num_quad_blocks <= 0 ==> 4*count < pow2_32;
            const2_1 == Mkfour(old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 1))), old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 0))));
            const4_3 == Mkfour(old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 3))), old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, add_wrap32(#nat32(4 * count), 2))));

            // AES reqs
            aes_reqs(alg, key, round_keys, keys_b, key_ptr, heap0, memLayout);

            // GCM reqs
            pclmulqdq_enabled;
            old_iv == reverse_bytes_quad32(icb_BE);
            iv_lower64 == lo64(old_iv);

            // GCTR progress
            length(slice(buffer128_as_seq(heap1, out_b), 0, 4 * old(num_quad_blocks))) > 0;
            hash == ghash_incremental0(reverse_bytes_quad32(h), old(hash), slice(buffer128_as_seq(heap1, out_b), 0, 4 * count));
            gctr_partial_def(alg, 4*count, old(buffer128_as_seq(heap1, plain_b)), buffer128_as_seq(heap1, out_b), key, icb_BE);
        decreases num_quad_blocks;
    {
        Aes_ctr_loop_body(alg, old(plain_ptr), old(out_ptr), old(num_quad_blocks), count, plain_b, out_b, key, round_keys, keys_b, icb_BE, old_iv, old(hash));
        count := count + 1;
    }

    Mov128(iv, const2_1);
    lemma_insrq_extrq_relations(iv, old_iv);
    Pinsrq(iv, iv_lower64, 0);
    //assert iv == Mkfour(old_iv.lo0, old_iv.lo1, old_iv.hi2, reverse_bytes_nat32(add_wrap32(icb_BE.lo0, 4*old(num_quad_blocks))));
    // Reversed: iv == Mkfour(r (r (add_wrap32(r(old_iv.hi3), 4*old(num_quad_blocks)))), r(old_iv.hi2), r(old_iv.lo1), r(old_iv.lo0));
    //              == Mkfour( add_wrap32(old(iv.lo0, 4*old(num_quad_blocks))), old(iv.lo2), old(iv.hi2), old(iv).hi3));
    //              == inc32(old(iv), 4*old(num_quad_blocks))
    reveal_reverse_bytes_quad32(iv);
    Pshufb(iv, mask);
    //assert iv == old(inc32(iv, 4*num_quad_blocks));
}
