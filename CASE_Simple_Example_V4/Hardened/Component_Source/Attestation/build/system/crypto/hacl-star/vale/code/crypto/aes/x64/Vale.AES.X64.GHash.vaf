include "../../../arch/x64/Vale.X64.InsBasic.vaf"
include "../../../arch/x64/Vale.X64.InsMem.vaf"
include "../../../arch/x64/Vale.X64.InsVector.vaf"
include "../../../arch/x64/Vale.X64.InsAes.vaf"
include "Vale.AES.X64.GF128_Mul.vaf"
include{:fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "Vale.Def.Words_s"
include{:fstar}{:open} "Vale.Def.Types_s"
include{:fstar}{:open} "Vale.Arch.Types"
include{:fstar}{:open} "Vale.AES.AES_s"
include{:fstar}{:open} "Vale.AES.GHash_s"
include{:fstar}{:open} "Vale.AES.GHash"
include{:fstar}{:open} "Vale.AES.GF128_s"
include{:fstar}{:open} "Vale.AES.GF128"
include{:fstar}{:open} "Vale.AES.GCTR_s"
include{:fstar}{:open} "Vale.AES.GCM_helpers"
include{:fstar}{:open} "Vale.Math.Poly2_s"
include{:fstar}{:open} "Vale.Poly1305.Math"
include{:fstar}{:open} "Vale.X64.Machine_s"
include{:fstar}{:open} "Vale.X64.Memory"
include{:fstar}{:open} "Vale.X64.State"
include{:fstar}{:open} "Vale.X64.Decls"
include{:fstar}{:open} "Vale.X64.QuickCode"
include{:fstar}{:open} "Vale.X64.QuickCodes"
include{:fstar}{:open} "Vale.X64.CPU_Features_s"

module Vale.AES.X64.GHash

#verbatim{:interface}{:implementation}
open Vale.Def.Opaque_s
open FStar.Seq
open Vale.Def.Words_s
open Vale.Def.Types_s
open Vale.Arch.Types
open Vale.AES.AES_s
open Vale.AES.GHash_s
open Vale.AES.GHash
open Vale.AES.GF128_s
open Vale.AES.GF128
open Vale.AES.GCTR_s
open Vale.AES.GCM_helpers
open Vale.Math.Poly2_s
open Vale.Poly1305.Math
open Vale.AES.X64.GF128_Mul
open Vale.X64.Machine_s
open Vale.X64.Memory
open Vale.X64.State
open Vale.X64.Decls
open Vale.X64.InsBasic
open Vale.X64.InsMem
open Vale.X64.InsVector
open Vale.X64.InsAes
open Vale.X64.QuickCode
open Vale.X64.QuickCodes
open Vale.X64.CPU_Features_s
#reset-options "--z3rlimit 30"
#endverbatim

#verbatim{:interface}
let get_last_slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
    last (slice s start_pos end_pos)
  else
    Mkfour 0 0 0 0

//let slice (s:seq quad32) (start_pos end_pos:int)  =
//  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
//    slice s start_pos end_pos
//  else
//    create 1 (Mkfour 0 0 0 0)
#endverbatim

///////////////////////////
// GHash
///////////////////////////

procedure Compute_Y0()
    {:quick}
    modifies xmm1; efl;
    requires sse_enabled;
    ensures xmm1 == Mkfour(0, 0, 0, 0);
{
    Pxor(xmm1, xmm1);
    lemma_quad32_xor();
}

procedure ReduceMul128_LE(ghost a:poly, ghost b:poly)
    {:quick}
    lets mask @= xmm8;
    reads mask;
    modifies
        efl; r12;
        xmm1; xmm2; xmm3; xmm4; xmm5; xmm6;
    requires
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        degree(a) <= 127;
        degree(b) <= 127;
        xmm1 == reverse_bytes_quad32(gf128_to_quad32(a));
        xmm2 == gf128_to_quad32(b);
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    ensures
        xmm1 == reverse_bytes_quad32(gf128_to_quad32(gf128_mul(a, b)));
{
    Pshufb(xmm1, mask);
    ReduceMulRev128(a, b);
    Pshufb(xmm1, mask);
}

procedure Compute_ghash_incremental_register()
    {:public}
    {:quick}
    lets input @= xmm2; io @= xmm1; mask @= xmm8; h @= xmm11;
    requires
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    reads
        h; mask;
    modifies
        io; efl; r12;
        xmm2; xmm3; xmm4; xmm5; xmm6;
    ensures
        io == ghash_incremental(reverse_bytes_quad32(h), old(io), create(1, old(input)));
{
    Pxor(io, input);    // Y_i := Y_{i-1} ^ x
    Mov128(xmm2, h);    // Move h into the register that ReduceMul128_LE expects

    ReduceMul128_LE(gf128_of_quad32(reverse_bytes_quad32(io)), gf128_of_quad32(h));    // io := Y_i * H
    ghash_incremental_reveal();
}

/* Seems to be unused
procedure Compute_ghash_incremental(ghost in_b:buffer128)
    {:quick}
    lets in_ptr @= rax; len @= rcx; io @= xmm1; mask @= xmm8; h @= xmm11;
    reads
        in_ptr; len; h; mask;
        heap0; memLayout;

    modifies
        rdx; r9; r12;
        io; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        len > 0 ==> validSrcAddrs128(heap0, in_ptr, in_b, len, memLayout, Secret);
        len > 0 ==> in_ptr  + 16 * len < pow2_64;
        len > 0 ==> buffer_length(in_b) == len;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);
        len > 0 ==> length(buffer128_as_seq(heap0, in_b)) > 0 /\ io == ghash_incremental(reverse_bytes_quad32(h), old(io), buffer128_as_seq(heap0, in_b));
{
    if (len != 0) {
        Mov64(rdx, 0);
        Mov64(r9, in_ptr);

        while (rdx != len)
            invariant
                //////////////////// Basic indexing //////////////////////
                0 <= rdx <= len;
                r9 == in_ptr + 16 * rdx;

                //////////////////// From requires //////////////////////
                // GHash reqs
                pclmulqdq_enabled && avx_enabled && sse_enabled;
                validSrcAddrs128(heap0, in_ptr, in_b, len, memLayout, Secret);
                in_ptr  + 16 * len < pow2_64;
                len > 0;
                buffer_length(in_b) == len;
                mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

                //////////////////// Postcondition goals //////////////////////
                rdx == 0 ==> io == old(io);
                rdx > 0 ==> io == ghash_incremental(reverse_bytes_quad32(h), old(io), slice(buffer128_as_seq(heap0, in_b), 0, rdx));

            decreases
                len - rdx;
        {
            Load128_buffer(heap0, xmm2, r9, 0, Secret, in_b, rdx);
            Pxor(io, xmm2);        // Y_i := Y_{i-1} ^ x
            Mov128(xmm2, h);       // xmm2 := H

            ReduceMul128_LE(gf128_of_quad32(reverse_bytes_quad32(xmm1)), gf128_of_quad32(xmm2));  // io := Y_i * H

            Add64(rdx, 1);
            Add64(r9, 16);
            ghash_incremental_reveal();
        }
    }
}

procedure Compute_ghash_incremental_partial(ghost in_b:buffer128)
    {:quick}
    lets in_ptr @= rax; len @= rcx; io @= xmm1; mask @= xmm8; h @= xmm11;
    reads
        in_ptr; len; h; mask;
        heap0; memLayout;

    modifies
        rdx; r9; r12;
        io; efl;
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        len > 0 ==> validSrcAddrs128(heap0, in_ptr, in_b, len, memLayout, Secret);
        len > 0 ==> in_ptr  + 16 * len < pow2_64;
        len > 0 ==> buffer_length(in_b) >= len;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        len == 0 ==> rdx == old(rdx) /\ r9 == in_ptr /\ xmm1 == old(xmm1) /\ io == old(io);
        let input := slice(buffer128_as_seq(heap0, in_b), 0, old(len));
        len > 0 ==> length(input) > 0 /\ io == ghash_incremental(reverse_bytes_quad32(h), old(io), input);
        r9 == in_ptr + 16 * len;
{
    Mov64(r9, in_ptr);
    if (len != 0) {
        Mov64(rdx, 0);

        while (rdx != len)
            invariant
                //////////////////// Basic indexing //////////////////////
                0 <= rdx <= len;
                r9 == in_ptr + 16 * rdx;

                //////////////////// From requires //////////////////////
                // GHash reqs
                pclmulqdq_enabled && avx_enabled && sse_enabled;
                validSrcAddrs128(heap0, in_ptr, in_b, len, memLayout, Secret);
                in_ptr  + 16 * len < pow2_64;
                len > 0;
                buffer_length(in_b) >= len;
                mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

                //////////////////// Postcondition goals //////////////////////
                rdx == 0 ==> io == old(io);
                rdx > 0 ==> io == ghash_incremental(reverse_bytes_quad32(h), old(io), slice(buffer128_as_seq(heap0, in_b), 0, rdx));

            decreases
                len - rdx;
        {
            Load128_buffer(heap0, xmm2, r9, 0, Secret, in_b, rdx);
            Pxor(io, xmm2);        // Y_i := Y_{i-1} ^ x
            Mov128(xmm2, h);       // xmm2 := H

            ReduceMul128_LE(gf128_of_quad32(reverse_bytes_quad32(xmm1)), gf128_of_quad32(xmm2));  // io := Y_i * H

            Add64(rdx, 1);
            Add64(r9, 16);
            ghash_incremental_reveal();
        }
    }
}

// Purely proof work to show that when there are no extra bytes, there's no work left to do
procedure Ghash_incremental_bytes_no_extra(
        ghost in_b:buffer128,
        ghost old_io:quad32,
        ghost old_in_ptr:nat64,
        ghost num_bytes:nat64,
        ghost io:quad32,
        ghost h:quad32)
    {:quick exportOnly}
    reads heap0; memLayout;
    requires
        // GHash reqs
        num_bytes > 0 ==> validSrcAddrs128(heap0, old_in_ptr, in_b, bytes_to_quad_size(num_bytes), memLayout, Secret);
        num_bytes > 0 ==> old_in_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        buffer_length(in_b) == bytes_to_quad_size(num_bytes);

        //len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);
        let input := slice(buffer128_as_seq(heap0, in_b), 0, num_bytes / 16);
        num_bytes > 0 ==> length(input) > 0 /\ io == ghash_incremental(h, old_io, input);

        // Extra reqs
        num_bytes % 16 == 0;
    ensures
        let input_bytes := slice(le_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b)), 0, num_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        num_bytes > 0 ==> length(input_quads) > 0 /\
                          io == ghash_incremental(h, old_io, input_quads);
{
    let num_blocks := num_bytes / 16;

    // Precondition variables
    let input := slice(buffer128_as_seq(heap0, in_b), 0, num_blocks);

    // Postcondition variables
    let input_bytes := slice(le_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b)), 0, num_bytes);
    let padded_bytes := pad_to_128_bits(input_bytes);
    let input_quads := le_bytes_to_seq_quad32(padded_bytes);

    // We want to show: input_quads == input

    no_extra_bytes_helper(buffer128_as_seq(heap0, in_b), num_bytes);
        // ==> input_bytes == slice (le_seq_quad32_to_bytes in_b) 0 num_bytes == le_seq_quad32_to_bytes in_b
    assert input_bytes == le_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b));
    assert pad_to_128_bits(input_bytes) == input_bytes;
    assert input_quads == le_bytes_to_seq_quad32(input_bytes);

    //slice_commutes_le_seq_quad32_to_bytes0(buffer128_as_seq(heap0, in_b), num_blocks);

//    input_quads == le_bytes_to_seq_quad32(le_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b)))
    //le_bytes_to_seq_quad32_to_bytes(slice(buffer128_as_seq(heap0, in_b), 0, num_blocks));
    le_bytes_to_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b));
//    input_quads == buffer128_as_seq(heap0, in_b)
//
    assert input == buffer128_as_seq(heap0, in_b);
}


/*
 * Note: More efficient options would be:
 *  a) Write a routine to build 16 masks for Pshufb, each of which implements a shift by 0-16 bytes.
 *     Ask GCM caller to pass in that buffer.
 *  b) Write a 16-way switch statement using Psrldq with immediates
 *     Note that a switch statement will likely be much more efficient than a 16-way if/else,
 *     but Vale currently doesn't support switch statements
 */
procedure Compute_pad_to_128_bits()
    {:quick}
    {:options z3rlimit(40)}
    lets io @= xmm2; num_bytes @= rax; tmp @= rcx; mask @= rdx;
    reads num_bytes;
    modifies io; tmp; mask; efl;
    requires 
        sse_enabled;
        0 < num_bytes < 16;
    ensures
        let padded_bytes := pad_to_128_bits(slice(le_quad32_to_bytes(old(io)), 0, old(num_bytes)));
        length(padded_bytes) = 16 && io = le_bytes_to_quad32(padded_bytes);
{
    lemma_poly_bits64();
    if (num_bytes < 8) {
        // Zero out the top 64-bits
        PinsrqImm(io, 0, 1, tmp);

        // Grab the lower 64 bits and zero-out 1-7 of the bytes
        Mov64(tmp, num_bytes);
        Shl64(tmp, 3);      // tmp == 8 (bits/byte) * num_bytes
        lemma_bytes_shift_power2(num_bytes); // ==>
        assert tmp == 8 * num_bytes;
        Mov64(mask, 1);
        Shl64(mask, tmp);
        Sub64(mask, 1);
        Pextrq(tmp, io, 0);
        let old_lower128 := tmp;
        And64(tmp, mask);
        lemma_bytes_and_mod(old_lower128, num_bytes); // ==>
        assert tmp == old_lower128 % (pow2(num_bytes * 8));

        // Restore the lower 64 bits
        Pinsrq(io, tmp, 0);

        lemma_lo64_properties();
        lemma_hi64_properties();
        pad_to_128_bits_lower(old(io), num_bytes);
    } else {
        assert num_bytes - 8 >= 0;      // TODO: Shouldn't need this with the new type checker
        // Grab the upper 64 bits and zero-out 1-7 of the bytes
        Mov64(tmp, num_bytes);
        Sub64(tmp, 8);      // Don't count the lower 64 bits
        Shl64(tmp, 3);      // tmp == 8 (bits/byte) * (num_bytes - 8)
        lemma_bytes_shift_power2(#nat64(num_bytes - 8));
        assert tmp == 8 * (num_bytes - 8);
        Mov64(mask, 1);
        Shl64(mask, tmp);
        Sub64(mask, 1);
        Pextrq(tmp, io, 1);
        let old_upper128 := tmp;
        And64(tmp, mask);
        lemma_bytes_and_mod(old_upper128, #nat64(num_bytes - 8)); // ==>
        // assert num_bytes - 8 >= 0 /\ tmp == old_upper128 % (pow2((num_bytes - 8) * 8));

        // Restore the upper 64 bits
        Pinsrq(io, tmp, 1);
        lemma_lo64_properties();
        lemma_hi64_properties();
        pad_to_128_bits_upper(old(io), num_bytes);
    }
}

procedure Ghash_incremental_bytes_register(
        ghost total_bytes:nat,
        ghost old_hash:quad32,
        ghost completed_quads:seq(quad32))
    {:quick}
    lets num_bytes @= rax; io @= xmm1; input_quad @= xmm2; mask @= xmm8; h @= xmm11;

    reads
        num_bytes; h; mask;

    modifies
        rcx; rdx; r12;
        io; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        io == ghash_incremental0(reverse_bytes_quad32(h), old_hash, completed_quads);

        // Extra reqs
        length(completed_quads) == total_bytes / 16;
        total_bytes < 16 * length(completed_quads) + 16;
        num_bytes == total_bytes % 16;
        total_bytes % 16 != 0;        // Note: This implies total_bytes > 0
        0 < total_bytes < 16 * bytes_to_quad_size(total_bytes);
        16 * (bytes_to_quad_size(total_bytes) - 1) < total_bytes;

    ensures
        let raw_quads := append(completed_quads, create(1, old(input_quad)));
        let input_bytes := slice(le_seq_quad32_to_bytes(raw_quads), 0, total_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        total_bytes > 0 ==> length(input_quads) > 0 /\
                          io == ghash_incremental(reverse_bytes_quad32(h), old_hash, input_quads);
{
    let final_quad := xmm2;

    Compute_pad_to_128_bits();
    let final_quad_padded := xmm2;

    Compute_ghash_incremental_register();

    lemma_ghash_incremental_bytes_extra_helper_alt(reverse_bytes_quad32(h), old_hash, old(io), io, completed_quads, final_quad, final_quad_padded, total_bytes);
}

procedure Ghash_incremental_bytes_extra(
        ghost in_b:buffer128,
        ghost orig_in_ptr:nat64,
        ghost old_io:quad32,
        ghost orig_num_bytes:nat64)
    {:quick}
    lets in_ptr @= r9; num_bytes @= rax; io @= xmm1; mask @= xmm8; h @= xmm11;

    reads
        num_bytes; h; mask; in_ptr; heap0; memLayout;

    modifies
        rcx; rdx; r12;
        io; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires/ensures
        orig_num_bytes > 0 ==> validSrcAddrs128(heap0, orig_in_ptr, in_b, bytes_to_quad_size(orig_num_bytes), memLayout, Secret);
    requires
        // GHash reqs
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        orig_num_bytes > 0 ==> orig_in_ptr  + 16 * bytes_to_quad_size(orig_num_bytes) < pow2_64;
        buffer_length(in_b) == bytes_to_quad_size(orig_num_bytes);
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        //len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);
        let input := slice(buffer128_as_seq(heap0, in_b), 0, orig_num_bytes / 16);
        io == ghash_incremental0(reverse_bytes_quad32(h), old_io, input);

        // Extra reqs
        let num_blocks := orig_num_bytes / 16;
        num_bytes == orig_num_bytes % 16;
        orig_num_bytes % 16 != 0;        // Note: This implies orig_num_bytes > 0
        0 < orig_num_bytes < 16 * bytes_to_quad_size(orig_num_bytes);
        16 * (bytes_to_quad_size(orig_num_bytes) - 1) < orig_num_bytes;
        in_ptr  == orig_in_ptr  + 16 * num_blocks;

    ensures
        let input_bytes := slice(le_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b)), 0, orig_num_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        orig_num_bytes > 0 ==> length(input_quads) > 0 /\
                          io == ghash_incremental(reverse_bytes_quad32(h), old_io, input_quads);
{
    let num_blocks := orig_num_bytes / 16;

    Load128_buffer(heap0, xmm2, in_ptr, 0, Secret, in_b, num_blocks);
    let final_quad := xmm2;

    Compute_pad_to_128_bits();
    let final_quad_padded := xmm2;

    Compute_ghash_incremental_register();

    lemma_ghash_incremental_bytes_extra_helper(reverse_bytes_quad32(h), old_io, old(io), io, buffer128_as_seq(heap0, in_b), final_quad, final_quad_padded, orig_num_bytes);
}

procedure Ghash_incremental_bytes(ghost in_b:buffer128)
    {:quick}
    lets in_ptr @= rax; num_bytes @= r11; io @= xmm1; mask @= xmm8; h @= xmm11;

    reads
        num_bytes; h; mask; heap0; memLayout;

    modifies
        in_ptr; rcx; rdx; r9; r12;
        io; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires/ensures
        num_bytes > 0 ==> validSrcAddrs128(heap0, old(in_ptr), in_b, bytes_to_quad_size(num_bytes), memLayout, Secret);
    requires
        // GHash reqs
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        num_bytes > 0 ==> in_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        num_bytes > 0 ==> buffer_length(in_b) == bytes_to_quad_size(num_bytes);
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        num_bytes == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);

        let input_bytes := slice(le_seq_quad32_to_bytes(buffer128_as_seq(heap0, in_b)), 0, num_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        num_bytes > 0 ==> length(input_quads) > 0 /\
            io == ghash_incremental(reverse_bytes_quad32(h), old(io), input_quads);
{
    let num_blocks := old(num_bytes) / 16;

    lemma_poly_bits64();

    if (num_bytes > 0) {
        Mov64(rcx, num_bytes);
        Shr64(rcx, 4);
        assert rcx == num_blocks;

        Compute_ghash_incremental_partial(in_b);

        Mov64(rax, num_bytes);
        And64(rax, 15);
        assert rax == num_bytes % 16;

        if (rax == 0) {
            Ghash_incremental_bytes_no_extra(in_b, old(io), old(in_ptr), old(num_bytes), io, reverse_bytes_quad32(h));
        } else {
            Ghash_incremental_bytes_extra(in_b, old(in_ptr), old(io), old(num_bytes));
        }
    }
}

procedure Ghash_core(ghost in_b:buffer128)
    {:quick}
    lets in_ptr @= rax; len @= rcx; output @= xmm1; mask @= xmm8; h @= xmm11;

    reads
        in_ptr; len; h; mask;
        heap0; memLayout;

    modifies
        rdx; r9; r12;
        output; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;
    requires
        // GHash reqs
        pclmulqdq_enabled && avx_enabled && sse_enabled;
        len > 0 ==> validSrcAddrs128(heap0, in_ptr, in_b, len, memLayout, Secret);
        len > 0 ==> in_ptr  + 16 * len < pow2_64;
        len > 0 ==> buffer_length(in_b) == len;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ output == old(output);
        len > 0 ==> length(buffer128_as_seq(heap0, in_b)) > 0 /\
                    output == ghash_LE(reverse_bytes_quad32(h), #ghash_plain_LE(buffer128_as_seq(heap0, in_b)));
{
    if (len != 0) {
        Compute_Y0();
        Compute_ghash_incremental(in_b);
        ghash_incremental_to_ghash(reverse_bytes_quad32(h), buffer128_as_seq(heap0, in_b));
    }
}
*/
